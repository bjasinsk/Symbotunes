model:
  model_type: muse-gan
  params:
    # vocab_size: 137
    # num_layers: 3
    # lstm_size: 512
    # dropout: 0.5
    # embedding_size: null
    # lr: 0.002
    # lr_decay: 0.97
    # lr_decay_start: 20

    optimizer:
      generator:
        learning_rate: 0.0002
        beta1: 0.5
        beta2: 0.999
      discriminator:
        learning_rate: 0.0002
        beta1: 0.5
        beta2: 0.999

    training:
          max_steps: 1000
          log_steps: 50
          save_checkpoints_steps: 500
          d_steps: 5
          g_steps: 1
          warmup_steps: 5

dataloaders:
  validation:
    name: "folk-rnn"
    root: "_data"
    split: "test"
    download: True
    data_type: "tokenized_ABC"
    transforms:
      - folk_rnn:    
    batch_size: 64
    num_workers: 16
    collate_fn: "pad_collate_single_sequence"

  train:
    - dataset:
        name: "for_musegan"
        root: "_data"
        split: "train"
        download: False
        data_type: "tokenized_ABC"
        transforms:
          - folk_rnn:
        
        batch_size: 64
        num_workers: 16
        collate_fn: "pad_collate_single_sequence"

lightning:
  trainer:
    benchmark: True
    max_epochs: 100
    gradient_clip_algorithm: "value"
    gradient_clip_val: 5

callbacks:
  - model_checkpoint:
      filename: "{epoch:06}"
      verbose: True
      save_last: True
  - checkpoint_every_n_steps:
      save_step_frequency: 1000
      prefix: "N-Step-Checkpoint"
      use_modelcheckpoint_filename: False
  - setup_callback:
  - cuda_callback:
